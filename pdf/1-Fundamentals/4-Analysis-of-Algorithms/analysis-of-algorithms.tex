\documentclass[12pt, a4paper]{article}

\usepackage[utf8]{inputenc}
% Limit the page margin to only 1 inch.
\usepackage[margin=1in]{geometry}

%Imports biblatex package
\usepackage[
backend=biber,
style=alphabetic
]{biblatex}
\addbibresource{../../algs4e.bib}

% Enables the `align' environment.
\usepackage{amsmath}
% Provides useful environments, such as:
% - \begin{proof} ...\end{proof}
\usepackage{amsthm}
% Enables using \mathbb{}, for example \mathbb{N} for the set of natural numbers.
\usepackage{amssymb}

% Allows using letters in enumerate list environment. Use, for example:
%\begin{enumerate}[label=(\alph*)]
% ...
%\end{enumerate}
\usepackage[inline]{enumitem}

% Enable importing external graphic files and provides useful commannds, like \graphicspath{}
\usepackage{graphicx}
% Images are located in a directory called images in the current directory.
\graphicspath{{./images/}}

% Make links look better by default.
% See: https://tex.stackexchange.com/questions/823/remove-ugly-borders-around-clickable-cross-references-and-hyperlinks
\usepackage[hidelinks]{hyperref}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={red!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}


% Code Listings. Source:
% https://stackoverflow.com/questions/3175105/inserting-code-in-this-latex-document-with-indentation
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=Java,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

\newcommand{\prob}{\text{P}}
%\newcommand{\complement}{\mathsf{c}}

% Define an environment called "ex" (for Exercise) so that I can do: \begin{ex}{1.5}...\end{ex}
\newenvironment{ex}[2][Exercise]
{\par\medskip\noindent \textbf{#1 #2.}}
{\medskip}

% Define a solution environment, similar to ex (exercise) environment.
\newenvironment{sol}[1][Solution]
{\par\medskip\noindent \textbf{#1.} }
{\medskip}

\begin{document}
	\noindent Sergio E. Garcia Tapia \hfill
	
	\noindent \emph{Algorithms} by Sedgewick and Wayne (4th edition) \cite{sedgewick_wayne}\hfill
	
	\noindent September 30th, 2024\hfill 
	\section*{1.4: Analysis of Algorithms}
	\begin{ex}{1}
		Show that the number of different triples that can be chosen from $n$ items is
		precisely $n(n-1)(n-2)/6$. \emph{Hint}: Use mathematical induction  or a counting argument.
	\end{ex}
	\begin{sol}
		\begin{proof}
			This is the problem of choosing a combination of $3$ out of $n$, which is given
			by $\binom{n}{3}$, and
			\begin{align*}
				\binom{n}{3} = \frac{n!}{3!(n-3)!}=\frac{n\cdot (n-1)\cdot (n-2)\cdot (n-3)!}{3!(n-3)!}  = \frac{n(n-1)(n-2)}{6}
			\end{align*}
		\end{proof}
	\end{sol}
	\begin{ex}{2}
		Modify \texttt{ThreeSum} to work properly even when the \texttt{int} values are so
		large that adding two of them might cause integer overflow.
	\end{ex}
	\begin{sol}
		There are two cases when it comes to overflow:
		\begin{enumerate}[label=(\roman*)]
			\item \emph{Positive overflow}. The sum exceeds \texttt{Integer.MAX\_VALUE}.
			If two terms sum to \texttt{Integer.MAX\_VALUE + 1}, overflow occurs, and
			the value wraps around to \texttt{Integer.MIN\_VALUE}. Thus, if
			$a + b = \texttt{Integer.MAX\_VALUE + 1}$ and $c = \texttt{Integer.MIN\_VALUE}$,
			then we have a valid sum. However, if $a+b$ sums to anything larger, then
			no value of $c$ will do because $c$ cannot be smaller than \texttt{Integer.MIN\_VALUE}.
			\item \emph{Negative overflow}. The sum of two negative numbers $a$ and $b$, yielding
			a value below \texttt{Integer.MIN\_VALUE}. In this case, it's impossible to have
			$a + b = c$ for any 32-bit two's complement integer $c$.
		\end{enumerate}
		See the \texttt{com.segarciat.algs4.ch1.sec4.ex02.ThreeSum} class.
	\end{sol}
	\begin{ex}{3}
		Modify \texttt{DoublingTest} to use \texttt{StdDraw} to produce
		plots like the standard and log-log plots in the text, rescaling as
		necessary so that the plot always fills a substantial portion of the
		window.
	\end{ex}
	\begin{sol}
		See my \texttt{com.segarciat.algs4.ch1.sec4.ex03.DoublingTest} class.
	\end{sol}
	\begin{ex}{4}
		Develop a table like the one on page 181 for \texttt{TwoSum}.
	\end{ex}
	\begin{sol}
		The \texttt{TwoSum} program referenced is:
		\begin{lstlisting}
public class TwoSum {
	public static int count(int[] a)
	{   // Count pairs that sum to 0;
		// A: The entire method body
		int n = a.length;
		int count = 0;
		// B: The outermost for loop and its body (not including the statement i = 0)
		for (int i = 0; i < n; i++)
			// C: The innermost for loop and its body (not  including the statement j = i + 1)
			for (int j = i + 1; j < n; j++)
				if (a[i] + a[j] == 0)
					// D: The number of times the if block is executed
					count++;
		return count;	
	}
}
		\end{lstlisting}
		The table in 181 is used to analyze the running time of by keeping track of
		the frequency of each statement block, that is, the number of times the
		block is executed:
		\begin{center}
			\begin{tabular}{cccc}
				\textbf{Statement block} & \textbf{Time in seconds} & \textbf{Frequency} & \textbf{Total time}\\
				\hline
				\texttt{D} & $t_0$ & $x$ (depends on input) & $t_0x$ \\
				\texttt{C} & $t_1$ & $\binom{n}{2}$ & $t_1\binom{n}{2}$ \\
				\texttt{B} & $t_2$ & $n$ & $t_2n$\\
				\texttt{A} & $t_3$ & $1$ & $t_3$
			\end{tabular}
		\end{center}
		Therefore, the grand total is:
		\begin{align*}
			t_1\binom{n}{2} + t_2n+t_3+t_0x &= t_1\frac{n(n-1)}{2} + t_2n+t_3+t_0x\\
			&=\frac{t_1}{2}n^2+\left(t_2-\frac{t_1}{2}\right)n+t_3+t_0x
		\end{align*}
		The tilde approximation (assuming $x$ is small) is
		\begin{align*}
			\sim\left(\frac{t_1}{2}\right)n^2
		\end{align*}
		Hence the order of growth is $n^2$.
	\end{sol}
	\begin{ex}{5}
		Give tilde approximations for the following quantities:
		\begin{enumerate}[label=(\alph*)]
			\item $n+1$
			\item $1 + 1/n$
			\item $(1 + 1/n)(1+2/n)$
			\item $(2n^3-15n^2)$
			\item $\lg(2n) / \lg n$
			\item $\lg(n^2+1) / \lg n$
			\item $n^{100}/2^n$
		\end{enumerate}
	\end{ex}
	\begin{sol}
		The definition of tilde approximation given on page 179 of \cite{sedgewick_wayne} says
		$\sim f(n)$ represents a function that, when divided by $f(n)$, approaches $1$ as $n$ grows.
		\begin{enumerate}[label=(\alph*)]
			\item $\sim n$
			\item $\sim 1$, since $1/n$ approaches $0$ as $n$ grows, and hence $1 + 1/n$ approaches
			$1$ as $n$ grows.
			\item $\sim 1$, similar to (b).
			\item $\sim 2n^3$
			\item Since $\lg(2n) = \lg(2) + \lg(n)$, this means $\lg(2n)/\lg n = \lg(2) / \lg(n) + 1$.
			Hence, this is again $\sim 1$.
			\item Since $\lg(n^2 + 1) \approx \lg(n^2)$, and $\lg(n^2) / \lg(n) = 2\lg(n)/\lg(n)$,
			we conclude that this is $\sim 2$.
			\item Here we can just say $\sim (n^{100}2^{-n})$.
		\end{enumerate}
	\end{sol}
	\begin{ex}{6}
		Give the order of growth (as a function of $n$) of the running times of each
		of the following code fragments:
		\begin{enumerate}[label=(\alph*)]
			\item
			\
			\begin{lstlisting}
		int sum = 0;
		for (int k = n; k > 0; k /= 2)
			for (int i = 0; i < k; i++)
				sum++;
			\end{lstlisting}
			\item
			\
			\begin{lstlisting}
		int sum = 0;
		for (int i = 1; i < n; i *= 2)
			for (int j = 0; j < i; j++)
				sum++;
			\end{lstlisting}
			\item
			\
			\begin{lstlisting}
		int sum = 0;
		for (int i = 1; i < n; i *= 2)
			for (int j = 0; j < n; j++)
				sum++;
			\end{lstlisting}
		\end{enumerate}
	\end{ex}
	\begin{sol}
		Let $m = \lfloor \lg n \rfloor + 1$. Then $m$ is the number of bits needed
		to represent $n$ in binary, and is the frequency of execution of the outer loop.
		In particular, $2^{m-1}\leq n < 2^m$.
		\begin{enumerate}[label=(\alph*)]
			\item For each value of \texttt{k}, the \texttt{i} loop block containing the statement
			\texttt{sum++} will execute \texttt{k} times, where $k = \lfloor n / 2^j\rfloor$
			for $0\leq j\leq m$. Thus the total number of times that \texttt{sum++} is executed
			is approximately given by
			\begin{align*}
				\sum_{j=0}^{m-1}\left\lfloor \frac{n}{2^j}\right\rfloor
				&<  \sum_{j=0}^{m-1}\left\lfloor\frac{2^m}{2^j}\right\rfloor\\
				&=\sum_{j=0}^{m-1}\frac{2^m}{2^j}\\
				&=2^m\sum_{j=0}^{m-1}\frac{1}{2^j}\\
				&=2^m\cdot \left(2-\frac{1}{2^{m-1}}\right)\\
				&=2^{m+1} - 2\\
				&=2(2^m - 1)\\
				&\leq 2(2^{\lg n + 1} - 1)\\
				&=2\cdot (2n -1 )
			\end{align*}
			The order of growth is $n$, or linear.
			\item This is similar to (a), but the analysis is much simpler:
			\texttt{i} takes on the values $2^k$ for $0\leq k<m$, and the \texttt{j} loop
			executes \texttt{i} times for each \texttt{i}. Thus the total number of times
			\texttt{sum++} runs is approximately:
			\begin{align*}
				\sum_{k=0}^{m - 1}2^k &= 2^m - 1 = 2^{\lg n + 1} - 1 \\
				&\leq 2^{\lg n + 1} - 1\\
				&= 2n - 1
			\end{align*}
			Thus the order of growth is $n$, or linear.
			\item The outer loop executes $m$ times, so again \texttt{i} takes on the
			values $2^k$ for $0\leq k<m$. For each \texttt{i}, the \texttt{j} loops executes \texttt{n}
			times. Thus the total number of times that \texttt{sum++} executes is $nm$ times,
			which is exactly $n(\lfloor \lg n + 1)\rfloor$ times. Hence the order of growth is linearithmic.
		\end{enumerate}
	\end{sol}
	\begin{ex}{7}
		Analyze \texttt{ThreeSum} under a cost model that counts arithmetic operations (and comparisons)
		involving the input numbers.
	\end{ex}
	\begin{sol}
		The arithmetic operations in \texttt{ThreeSum} are all additions. The comparison operations
		are less than (\texttt{<}) and equal to (\texttt{==}).
		
		Altogether, there are $n + 1$ comparisons from \texttt{i < n}, $\binom{n}{2} + 1$ comparisons
		from \texttt{j < n} (once for each pair $(i, j)$), $\binom{n}{3} + 1$ comparisons from \texttt{k < n}, and
		$\binom{n}{3}$ comparisons for \texttt{a[i] + a[j] + a[k] == 0} (one for each triple $(i,j,k)$).
		
		There are $n$ additions from \texttt{i++}, $\binom{n}{2}$ additions for \texttt{j++},
		$\binom{n}{3}$ additions for \texttt{k++}, $2\cdot \binom{n}{3}$ additions for
		\texttt{a[i] + a[j] + a[k]} (since it involves two additions for each triple),
		and the number of times \texttt{count++} executes is indeterminate since it depends
		precisely on how many times the control expression of the \texttt{if} evaluates to \texttt{true}.
		
		Overall, then, the cost is dominated by the statements in the innermost \texttt{k} loop.
		Therefore, we can say that \texttt{ThreeSum} uses about $5\binom{n}{3}$ or $\sim \frac{5}{6}n^3$
		arithmetic operations (including comparisons), and hence its order of growth is cubic under
		a cost  model that counts arithmetic operations and comparisons.
	\end{sol}
	\begin{ex}{8}
		Write a program to determine the number of pairs of values in an input file that
		are equal. If your first try is quadratic, think again and use \texttt{Arrays.sort()}
		to develop a linearithmic solution.
	\end{ex}
	\begin{sol}
		See the \texttt{com.segarciat.algs.ch1.sec4.ex08.EqualNumberPairs} class. It uses
		\texttt{Arrays.sort()}, which has a linearithmic order of growth. Then it uses the
		fact that equal numbers are adjacent to each other to compute the frequency of
		occurrence of each number. If a number has a frequency $f$, then there are
		$\binom{f}{2}$ equal pairs corresponding to that number. This loop operates
		in linear time since it performs at most a constant number of operations
		in each iteration, and it iterates $n$ times, where $n$ is the number of
		items in the file.
	\end{sol}
	\begin{ex}{9}
		Give a formula to predict the running time of a program for a problem of size $n$
		when doubling experiments have shown that the doubling factor is $2^b$ and the
		running time for problems of size $n_0$ is $t$.
	\end{ex}
	\begin{sol}
		Let $T(n)$ be the running time of the program. Since the doubling factor is $2^b$,
		we know that $T(n)$ has an order of growth approximately $n^b$, as claimed in \cite{sedgewick_wayne}
		on page 192 (Section 1.4). Hence,
		\begin{align*}
			T(n) &\sim n^b\\
			&=\left(\frac{n}{n_0}n_0\right)^b\\
			&=\left(\frac{n}{n_0}\right)^b n_0^b\\
			&\sim \left(\frac{n}{n_0}\right)^b T(n_0)
		\end{align*}
		Since $T(n_0)=t$, we can predict $T(n)$ to be approximately $\left(\frac{n}{n_0}\right)^bt$.
	\end{sol}
	\begin{ex}{10}
		Modify binary search so that it always returns the element with the smallest index that
		matches the search element (and still guarantees logarithmic running time).
	\end{ex}
	\begin{sol}
		See the \texttt{com.segarciat.algs4.ch1.sec4.ex10.BinarySearch} class. I implemented an
		\texttt{indexOf()} method that is similar to my \texttt{rank()} implementation in
		Exercise 1.1.29; see \texttt{com.segarciat.algs4.ch1.sec1.ex29.BinarySearch}.
		
		In Exercise 1.1.29, the return value is a number that is between \texttt{0} and \texttt{a.length},
		inclusive. There are two cases:
		\begin{enumerate}[label=(\roman*)]
			\item When the \texttt{key} exists in the array, the return value \texttt{a[lo]}
			has value \texttt{key}. Moreover, \texttt{lo} is the smallest index such that
			\texttt{a[lo]} equals \texttt{key}.
			\item When \texttt{key} is not in the array, then either \texttt{lo} is \texttt{a.length}
			(indicating that \texttt{key} exceeds every value in the array) or \texttt{lo} is between
			\texttt{0} and \texttt{a.length - 1}, but \texttt{a[lo]} is not equal to \texttt{key}.
		\end{enumerate}
		My implementation for this exercise adapts the code my \texttt{rank()} method in 1.1.29
		with these considerations.
	\end{sol}
	\begin{ex}{11}
		Add an instance method \texttt{howMany()} to \texttt{StaticSETofInts} (page 99)
		that finds the number of occurrences of a given key in time proportional to
		$\log n$ in the worst case.
	\end{ex}
	\begin{sol}
		My implementation again adapts the code from Exercise 1.1.29, namely the \texttt{rank()} and
		\texttt{rankGe()} methods. Both adaptations run in $\log n$ in the worst case.
		The \texttt{howMany()} calls each once, and hence it completes in time proportional to
		$\log n$ as well.
	\end{sol}
	\begin{ex}{12}
		Write a program that, given two sorted arrays of $n$ \texttt{int} values, prints all
		elements that appear in both arrays, in sorted order. The running time for your program
		should be proportional to $n$ in the worst case.
	\end{ex}
	\begin{sol}
		See the \texttt{com.segarciat.algs4.ch1.sec4.ex12.PrintTwoSortedArrays} class.
	\end{sol}
	\begin{ex}{13}
		Using the assumptions developed in the text, give the amount of memory needed to
		represent an object of each of the following types:
		\begin{enumerate}[label=(\alph*)]
			\item \texttt{Accumulator}
			\item \texttt{Transaction}
			\item \texttt{FixedCapacityStackOfStrings} with capacity \texttt{capacity}
			and \texttt{n} entries.
			\item \texttt{Point2D}
			\item \texttt{Interval1D}
			\item \texttt{Interval2D}
			\item \texttt{Double}
		\end{enumerate}
	\end{ex}
	\begin{sol}
		According to page 201 (Section 1.4) in \cite{sedgewick_wayne}, each object has
		an associated overhead of 16 bytes. Also, memory usage is typically padded to be a multiple
		of 8 bytes on a 64-bit machine. Hereafter I assume a 64-bit machine:
		\begin{enumerate}[label=(\alph*)]
			\item An \texttt{Accumulator} object requires 16 bytes of overhead, 8 bytes for the
			\texttt{sum} instance variable of type \texttt{double}, 4 bytes for the \texttt{n} instance
			variable of type \texttt{int}, and 4 bytes of padding. The grand total is 32 bytes.
			\item A \texttt{Transaction} object requires 16 bytes of overhead, 8 bytes
			for the \texttt{who} reference variable of type \texttt{String},
			8 bytes for the \texttt{when} instance variable of type \texttt{Date}, and
			8 bytes for the \texttt{amount} instance variable of type \texttt{double}.
			These add up to 40 bytes. We also account for the cost plus the cost of a \texttt{String}
			and a \texttt{Date} object. For a \texttt{String}, assuming Java 7 and later,
			the cost is $56 + 2n$ bytes (see page 202 on \cite{sedgewick_wayne}), where $n$ is
			the number of characters in the string. For a \texttt{edu.princeton.cs.algs4.Date}
			object, the cost is 32 bytes (see page 201 on \cite{sedgewick_wayne}). Altogether,
			this amounts to $40 + 56+2n + 32=128+2n$ bytes, where $n$ is the number of characters
			in the \texttt{who} instance variable.
			\item A \texttt{FixedCapacityStackOfStrings} object requires 16 bytes of overhead,
			4 bytes for the \texttt{n} instance variable of type \texttt{int}, 8 bytes for
			the \texttt{a} instance variable of type \texttt{String[]}, and 4 bytes of padding
			to be 32 bytes. According to page 202 in \cite{sedgewick_wayne}, an array of size
			\texttt{capacity} takes up $24 + 8\cdot \texttt{capacity}$ bytes, plus the cost
			of the \texttt{n} string entries. If we let $m$ be the value of \texttt{capacity},
			this means a total of $56 + 8m$ plus the cost of the \texttt{n} objects of type \texttt{String},
			whose lengths (and hence memory requirements) vary.
			\item A \texttt{Point2D} object requires 16 bytes of overhead, and 8 bytes for each
			of the instance variables \texttt{x} and \texttt{y} of type \texttt{double}. The
			grand total is $32$ bytes.
			\item An \texttt{Interval1D} object requires 16 bytes of overhead, and 8 bytes
			for each of the instance variables \texttt{min} and \texttt{max} of type \texttt{double}.
			The grand total is $32$ bytes.
			\item An \texttt{Interval2D} object requires 16 bytes of overhead, 8 bytes
			for each of the instance \texttt{x} and \texttt{y} of type \texttt{Interval1D},
			and $32$ bytes for the cost of each \texttt{Interval1D} object. The total is
			$96$ bytes.
			\item A \texttt{Double} object requires 16 bytes of overhead and 8 bytes for its instance
			variable of type \texttt{double}. Overall this takes up 24 bytes.
		\end{enumerate}
	\end{sol}
	\begin{ex}{1.4.14}
		Develop an algorithm for the $4$-\emph{sum} problem.
	\end{ex}
	\begin{sol}
		See the \texttt{com.segarciat.algs4.ch1.sec4.ex14.FourSum} class. I did not consider
		overflow.
	\end{sol}
	\begin{ex}{1.4.15}
		\emph{Faster 3-sum}. As a warmup, develop an implementation \texttt{TwoSumFaster} that
		uses a \emph{linear} algorithm to count the pairs that sum to zero after the array
		is sorted (instead of the binary-search-based linearithmic algorithm). Then apply
		a similar idea to develop a quadratic algorithm for the 3-sum problem.
	\end{ex}
	\begin{sol}
		I implemented \texttt{TwoSumFaster} by scanning from opposite sides of the array.
		I accounted for the possibility that duplicates may exist by including an inner
		loop to count duplicates and applying the multiplication principle of counting.
		Overall, the \texttt{TwoSumFaster} algorithm is still linearithmic because
		it uses \texttt{Arrays.sort()}, which is linearithmic. However, this still satisfies
		the constraints of the exercises which requires a linear algorithm \emph{after} sorting.
		I then implemented \texttt{ThreeSumFaster} by applying the algorithm in
		\texttt{TwoSumFaster} a total of $n$ times, where $n$ is the array length,
		once for each $i$ between $0$ and $n-1$. The algorithm in \texttt{ThreeSumFaster}
		is quadratic: the \texttt{Arrays.sort()} is linear, while the \texttt{i} loop
		containing the scan from both ends is quadratic.
	\end{sol}
	\begin{ex}{1.4.16}
		\emph{Closest pair (in one dimension)}. Write a program that, given an array \texttt{a[]}
		of $n$ \texttt{double} values, finds a \emph{closest pair}: two values whose difference
		is no greater than the difference of any other pair (in absolute value). The running time
		of your program should be linearithmic in the worst case.
	\end{ex}
	\begin{sol}
		See \texttt{com.segarciat.algs4.ch1.sec4.ex16.ClosestPair1D}.
	\end{sol}
	\begin{ex}{1.4.17}
		\emph{Farthest pair (in one dimension)}. Write a program that, given an array \texttt{a[]}
		of $n$ \texttt{double} values, finds a \emph{farthest pair}: two values whose difference is
		no smaller than the difference of any other pair (in absolute value). The running time
		of your program should be linear in the worst case.
	\end{ex}
	\begin{sol}
		See the \texttt{com.segarciat.algs4.ch1.sec4.ex17.FarthestPair1D} class.
	\end{sol}
	\begin{ex}{1.4.18}
		\emph{Local minimum of an array}. Write a program that, given an array \texttt{a[]}
		of $n$ distinct integers, finds a \emph{strict local minimum}: an entry \texttt{a[i]}
		that is strictly less than its neighbors. Each internal entry (other than \texttt{a[0]}
		and \texttt{a[n-1]}) has 2 neighbors. Your program should use $\sim \lg n$ compares in
		the worst case.
	\end{ex}
	\begin{sol}
		First we can prove that there must be a local minimum in an array of distinct values.
		\begin{proof}
			The proof is by contradiction. That is, suppose that no local minimum exists.
			Then \texttt{a[0]} is not a local minimum, so it is larger than its neighbor \texttt{a[1]}.
			Since \texttt{a[1]} is a not a local minimum, it is larger than one of its
			neighbors. Since it is smaller than \texttt{a[0]}, it must be larger than
			\texttt{a[2]}. In this way, we obtain a decreasing sequence, and conclude that
			the array is sorted in reverse order. But then \texttt{a[n-1]} is less than
			its left neighbor, and it would have to be the local minimum. This is a contradiction.
		\end{proof}
		I implemented an algorithm that is similar to binary search, computing the index of
		the middle element each time, and comparing it against its neighbors (2 comparisons).
		Since the intervals cut the search space in half each time, the number of compares
		used is $\sim 2 \lg n$.
		
		The algorithm begins by checking whether the first or the last element is a local minimum.
		These are a special case because they are the only elements with a single neighbor.
		If the array has only two elements, then one of these must be the local minimum because
		all elements are distinct.
		
		Suppose that the array had more than two elements. The algorithm proceeds by setting
		\texttt{lo} to 1 and \texttt{hi} to \texttt{a.length - 2}. At this point, \texttt{a[lo-1]}
		is larger than \texttt{a[lo]} because \texttt{a[lo-1]} is \texttt{a[0]}, which is not
		the local minimum, as determined by the previous check.Similarly, \texttt{a[hi+1]} is
		larger than \texttt{a[hi]} by the same reasoning. The algorithm continues its search
		for a local minimum in the index interval \texttt{lo..hi} by checking whether
		the element at index \texttt{mid = lo + (hi - lo) / 2} is a local minimum. There are
		three cases:
		\begin{enumerate}[label=(\roman*)]
			\item If \texttt{a[mid]} is larger than \texttt{a[mid+1]}, this means \texttt{a[mid]}
			is not a local minimum, but \texttt{a[mid+1]} \emph{may} be a local minimum.
			We set \texttt{lo = mid + 1} and continue the search in \texttt{mid+1..hi}.
			\item Otherwise if \texttt{a[mid]} is larger than \texttt{a[mid-1]}, this
			means again that \texttt{a[mid]} is not a local minimum but \texttt{a[mid-1]}
			\emph{may} be a local minimum, so we set \texttt{hi = mid - 1} and continue the
			search in the index interval \texttt{lo..mid-1}.
			\item Otherwise, we have found a local minimum.
		\end{enumerate}
		The search loops this way and ends when \texttt{lo > hi}. To see that the algorithm is
		able to find a local minimum this way, note that it maintains the invariant that
		\texttt{a[lo-1] > a[lo] \&\& a[hi+1] > a[hi]} is \texttt{true} at all stages.
		Indeed, the condition holds before entering the loop, and whenever cases (i) and (ii)
		update \texttt{lo} or \texttt{hi}, the invariant still holds. Suppose that the loop
		ends because \texttt{lo > hi}. This happens when \texttt{lo = hi + 1}. The invariant
		assures us that \texttt{a[lo-1] > a[lo] \&\& a[hi+1] > a[hi]}. Since \texttt{a[lo-1]} is
		\texttt{a[hi]}, and \texttt{a[hi+1]} is \texttt{a[lo]}, this would say that
		\texttt{a[hi] > a[lo] \&\& a[lo] > a[hi]} is \texttt{true}, which is impossible
		because there is a total ordering for integers. We conclude that the condition
		\texttt{lo > hi} can never be reached, and hence, the program will end by returning
		the index \texttt{mid} of a local minimum.
	\end{sol}
	\begin{ex}{19}
		\emph{Local minimum of a matrix}. Given an $n$-by-$n$ array \texttt{a[]} of $n^2$ distinct
		integers, design an algorithm that finds a \emph{strict local minimum}: an entry
		\texttt{a[i][j]} that is strictly less than its neighbors. Internal entries have 4 neighbors;
		entries on an edge have 3 neighbors; entries on a corner have 2 neighbors. The running time
		of your program should be proportional to $n$ in the worst case, which means you cannot
		afford to examine all $n^2$ entries.
	\end{ex}
	
	\begin{sol}
		I failed to obtain a linear solution. I implemented a linearithmic solution by using the
		hint on the book website. It is similar to my solution in Exercise 19. It works by
		finding the minimum entry in the middle row, and then checking against its neighbors
		in the same column. The row search space is cut in half each time, and the search ends
		when its size is $0$. A similar mathematical analysis to 1.4.18 would show this algorithm
		works.
	\end{sol}
	
	\begin{ex}{20}
		\emph{Bitonic search}. An array is \emph{bitonic} if it is comprised of an increasing sequence
		of integers followed immediately by a decreasing sequence of integers. Write a program
		that, given a bitonic array of $n$ distinct \texttt{int} values, determines whether a given
		integer is in the array. Your program should use $\sim 3\lg n$ compares in the worst
		case. \emph{Extra credit}: use only $\sim 2\lg n$ compares in the worst case.
	\end{ex}
	\begin{sol}
		Suppose that \texttt{a[]} is bitonic and let \texttt{n} by its length. If the
		search key is \texttt{a[0]} or \texttt{a[n-1]}, then there's nothing else to do.
		
		Otherwise, we search for the key in the interval \texttt{1..n-2}. Since all of the entries
		are distinct, there must be a maximum entry. Finding the maximum itself can also be done via
		binary search. We simply start with the interval \texttt{1..n-2}. If the middle entry is less
		than its predecessor, then we've gone too far and fallen into the decreasing sequence, so we
		continue our search on the left. Otherwise, if the key is less than its successor in the sequence,
		then we are in the increasing sequence, so we search on the right.
		
		Suppose now that \texttt{i} is the index of the maximum entry. If the search key equals the
		maximum entry, then our search is done. Otherwise, note that the maximum satisfies \texttt{a[i] > a[i+1]}
		and \texttt{a[i] > a[i-1]}. Since the array is bitonic, this means that entries in the range
		\texttt{0..i-1} form the increasing sequence, and entries in the range \texttt{i+1..n-1} form an
		decreasing sequence. Thus, we can continue the search for the key on the disjoint sequences that
		do not include index \texttt{i} of the maximum by using binary search.
	\end{sol}
	\begin{ex}{1.4.21}
		\emph{Binary search on distinct values}. Develop an implementation of binary search
		for \texttt{StaticSETofInts} (see page 99) where the running time of \texttt{contains()}
		is guaranteed to be $\sim \log d$, where $d$ is the number of different integers in the
		array given as argument to the constructor.
	\end{ex}
	\begin{ex}{1.4.23}
		\emph{Binary search with duplicates}. Modify \texttt{indexOf()} (page 47) so that it
		returns the index of the \emph{first} (or \emph{last}) entry equal to the query key.
		The order of growth of your method should be $\log n$ in the worst case, even if there
		are many entries in the array equal to the query key.
	\end{ex}
	\begin{sol}
		This is effectively the same as the same as Exercise 1.4.10, so I will omit the code.
	\end{sol}
	\begin{ex}{1.4.26}
		\emph{3-collinearity}. Suppose that you have an algorithm that takes as input $n$
		distinct points in the plane and can return the number of triples that fall on
		the same line. Show that you can use this to solve the 3-sum problem.
		\emph{Strong hint}: Use algebra to show that $(a,a^3)$, $(b,b^3)$, and $(c,c^3)$
		are collinear if and only if $a+b+c=0$.
	\end{ex}
	\begin{sol}
		\begin{proof}
			We can begin by proving the claim in the hint. Suppose that $a,b,c$ are distinct
			and $(a,a^3)$, $(b,b ^3)$, and $(c,c^3)$ are collinear. The line cannot be
			vertical because otherwise all $x$ coordinates are equal, implying that
			$a$, $b$, and $c$ are distinct, contrary to assumption.
			
			Suppose that the points are on a line with slope $m$. Then
			Then
			\begin{align*}
				m &= \frac{c^3-b^3}{c-b} = (c^2+cb+b^2)\\
				m &= \frac{c^3-a^3}{c-a} = (c^2+ca+a^2)\\
				m &= \frac{b^3-a^3}{b-a} = (b^2+ba+a^2)
			\end{align*}
			where we have divided by common factors because all three integers are distinct.
			Equating the first two equations gives
			\begin{align*}
				c^2+cb+b^2 &= c^2+ca+a^2\\
				cb+b^2&=ca+a^2\\
				b^2-a^2 &= ca - cb\\
				(b-a)(b+a) &= c(a-b)\\
				b+a &= -c\\
				a+b+c &= 0
			\end{align*}
			Again we were able to divide by the common factor of $b-a$ because the
			integers are all distinct. We have shown that $a+b+c=0$. By working backwards
			(retracting steps) we could show the converse.
			
			Now, given this algorithm and the previously proven result, we could solve the
			3-sum problem by creating, $n$ triples of the form $(\alpha,\alpha^3)$, where $\alpha$ is
			one of the distinct $n$ numbers. When the algorithm finds the there collinear
			points, we know their $x$-coordinates sum to $0$.
		\end{proof}
	\end{sol}
	\begin{ex}{1.4.37}
		\emph{Autoboxing performance penalty}. Run experiments to determine the performance
		penalty on your machine for using autoboxing and unboxing.  Develop an implementation
		\texttt{FixedCapacityStackOfInts} and use a client such as \texttt{DoublingRatio}
		to compare its performance with the generic \texttt{FixedCapacityStack<Integer>},
		for a large number of \texttt{push()} and \texttt{pop()} operations.
	\end{ex}
	\begin{sol}
		For large $n$, the version that requires autoboxing to \texttt{Integer} requires around $50$
		times as long to run as the primitive \texttt{int} version.
	\end{sol}
	\begin{ex}{1.4.43}
		\emph{Resizing array versus linked lists}. Run experiments to validate the hypothesis
		that resizing arrays are faster than linked lists for stacks (see Exercise 1.4.35
		and Exercise 1.4.36). Do so by developing a version of \texttt{DoublingRatio}
		that computes the ratio of the running times of the two programs.
	\end{ex}
	\begin{sol}
		I observed that resizing implementation tends to run about 3 times as fast as the linked list
		implementation.
	\end{sol}
	\pagebreak
	\printbibliography
\end{document}